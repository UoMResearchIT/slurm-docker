SlurmctldHost=slurm

AuthType=auth/munge

# 
#MailProg=/bin/mail 
MpiDefault=none
#MpiParams=ports=#-# 
#ProctrackType=proctrack/cgroup
ProctrackType=proctrack/linuxproc
ReturnToService=1
SlurmctldPidFile=/run/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/run/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser=slurm
#SlurmdUser=root 
StateSaveLocation=/state
SwitchType=switch/none
TaskPlugin=task/affinity
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
#
# SCHEDULING 
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
# 
# 
# LOGGING AND ACCOUNTING 
#AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageType=accounting_storage/filetxt
AccountingStorageLoc=/var/log/slurm-llnl/accounting.log
ClusterName=cluster

JobCompType=jobcomp/filetxt
JobCompLoc=/state/jobcomp.log

JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/linux
#SlurmctldDebug=info 
#SlurmctldDebug=verbose
#DebugFlags=gres
#SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
#SlurmdDebug=info 
#SlurmdDebug=debug2
#SlurmdLogFile=
# 
# 

# This is needed because users on the cluster need to submit larger
# arrays than the default limit
MaxArraySize=10001

# This is necessary because SLURM remembers all jobs for MinJobAge
# seconds, so if very large numbers of jobs (or array tasks) are
# submitted, even if they have run, there won't be space for any more,
# and users get the error "#sbatch: error: Slurm temporarily unable to
# accept job, sleeping and retrying".  Setting this too high might
# cause SLURM to run out of memory, in which case the 1 GB memory
# allocated to the slurmcontroller VM will need to be increased.
MaxJobCount=50000

# COMPUTE NODES
NodeName=slurm State=idle

PartitionName=compute Nodes=slurm Default=YES MaxTime=INFINITE State=UP
